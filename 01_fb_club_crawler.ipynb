{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【NLP 自然語言處理】2024 FaceBook社團貼文爬蟲 Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selenium 爬蟲"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 01. 瀏覽器設定：允許所有網站通知行為\n",
    "\n",
    "`profile.default_content_setting_values.notifications`：這是 Chrome 設定中負責控制網站通知行為的部分。\n",
    "- 1：允許對於網站發出的通知。\n",
    "\n",
    "- 2：阻擋通知。\n",
    "\n",
    "![notifications](./image/notification.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "\n",
    "chrome_options = webdriver.ChromeOptions()\n",
    "\n",
    "# 允許所有網站通知行為\n",
    "chrome_options.add_experimental_option(\n",
    "    \"prefs\", \n",
    "    {\n",
    "        \"profile.default_content_setting_values.notifications\": 1\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 02. 啟動 webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = webdriver.Chrome(options = chrome_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 03. 設定視窗大小\n",
    "\n",
    "不同的視窗大小會影響網頁排版樣式，間接影響爬蟲的流程，事先設定好可以避免因排版問題而導致爬蟲錯誤。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.set_window_size(800, 800)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 04. 網址導向\n",
    "\n",
    "利用 `get` 可將 Webdriver 導向到指定的網站頁面。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "login_url = 'https://www.facebook.com/login/device-based/regular/login/?login_attempt=1&next=https%3A%2F%2Fwww.facebook.com'\n",
    "driver.get(login_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 05. 輸入 Input \n",
    "\n",
    "利用 `find_element` 可以抓到網頁的指定的位置：\n",
    "\n",
    "- 第一個參數：指定 Selector 來定向到目標節點：輸入框、按鈕。例如：By.CSS_SELECTOR （使用 CSS Selector）\n",
    "\n",
    "- 第二個參數：目標節點的位置。例如：#email_container input （目標節點的 CSS 位置）\n",
    "\n",
    "可用於輸入搜尋框、登入輸入帳密...等"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Email 輸入框\n",
    "\n",
    "由下圖可以發現 Email 輸入框對應的節點是標籤 input，但不是只有 Email 輸入框的標籤是 input。\n",
    "\n",
    "因此，我可以從標籤 input 的前一層（父級）來去指定，我們抓取前一層的屬性 id。在 CSS Selector 的表示為 `#email_container`。\n",
    "\n",
    "如果要指定屬性 id 為 `email_container` 底下的標籤 input，則空格加上下一層（子級）的 Selector，所以就會是 `#email_container input`\n",
    "\n",
    "![Email](./image/login.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Password 輸入框\n",
    "\n",
    "Password的概念相同，不同的是，我們先嘗試選擇屬性 class，再加上標籤 input。\n",
    "\n",
    "如果要使用 CSS Selector 來指定屬性 class，則要加上 `.`，例如： `._55r1`。\n",
    "\n",
    "![Email](./image/pass.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但是，可以注意到在這個例子的屬姓 class 是 `_55r1 _1kbt`，中間有了空格。這裡並不是 classname 可以包含空格，而是這裡存在兩個 classname。（理論上沒有上限，屬性值可以有複數個名稱，會以空格來區隔）。\n",
    "\n",
    "根據這裡包含的 classname，可以有幾種指定方式：\n",
    "\n",
    "- 指定其中一個 classname：`._55r1`、`._1kbt`。\n",
    "\n",
    "- 指定所有 classname：`._55r1._1kbt`（兩個 classname 位在同一個標籤下，不存在從屬關係，因此 CSS Selector 不須空格）。\n",
    "\n",
    "加上標籤 input 後，就會變成 `._55r1._1kbt input`（`input` 在 `_55r1 _1kbt` 下，所以 CSS Selector 須空格）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "\n",
    "# 建議新建一個新的帳號，避免被鎖\n",
    "facebook_mail = 'YOUR_FACEBOOK_MAIL'\n",
    "facebook_password = 'YOUR_FACEBOOK_PASSWORD'\n",
    "\n",
    "email_element = driver.find_element(By.CSS_SELECTOR,'#email_container input')\n",
    "password_element = driver.find_element(By.CSS_SELECTOR,'._55r1._1kbt input')\n",
    "\n",
    "email_element.send_keys(facebook_mail)\n",
    "password_element.send_keys(facebook_password)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 06. 點擊按鈕"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Login 按鈕\n",
    "\n",
    "由於 id 屬性在這個網站是唯一的，`loginbutton`。因此 CSS Selector 直接設置為 `#loginbutton`即可。\n",
    "\n",
    "![Email](./image/btn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "login_button = driver.find_element(By.CSS_SELECTOR, '#loginbutton')\n",
    "\n",
    "login_button.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前往指定社團頁面"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.facebook.com/groups/traveler168'\n",
    "\n",
    "driver.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 07. 頁面滾動\n",
    "\n",
    "由於動態網頁的網頁內容是透過Javascript渲染而成，因此進入網頁後不會一次渲染出所有內容，例如：\n",
    "\n",
    "- FaceBook 社團貼文\n",
    "- Instagram 搜尋結果頁\n",
    "- Google Maps 景點評論\n",
    "\n",
    "上面提到的例子，都必須透過使用者不斷滾動頁面來載入更多內容，而在爬蟲時，我們就必須模擬像這樣的滾動行為，來取得更多資料。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`execute_script` 可以幫助我們在 Webdriver 上執行 Javascript，如此一來便能夠做出更細緻、進階的操作，包含但不限於：頁面滾動、處理彈出窗口、修改網頁元素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們可以寫一個迴圈，讓網站可以連續下滑3次。\n",
    "\n",
    "註：由於網頁 DOM 節點的載入需要時間，滾動頁面時中間需間隔一段時間，否則在節點載入完成前 Javascript 就會先執行完畢。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "counter = 0\n",
    "while counter <= 3:\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    # 等待 0.5 秒（確保網頁節點有載入，可根據需求調整等待時間）\n",
    "    time.sleep(0.5)\n",
    "    counter += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 08. 關閉 Webdriver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 註解避免 Webdriver 關閉，這樣就可以接續往下執行\n",
    "# driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【NLP 自然語言處理】2024 FaceBook社團貼文爬蟲 Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install selenium pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 Facebook 社團爬蟲的下半章節，將開始實際爬取社團內的貼文內容。爬取的資訊包含：\n",
    "\n",
    "- 作者\n",
    "\n",
    "- 讚數\n",
    "\n",
    "- 時間\n",
    "\n",
    "- 連結\n",
    "\n",
    "- 貼文內容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 解析網頁內容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 Python當中，我們可以借助 BeautifulSoup 來去解析網頁內容。\n",
    "\n",
    "延續上一篇文，`driver` 會取得瀏覽器頁面的原始 HTML 內容（就如我們實際在畫面上到的那樣）。而 `driver` 帶有 `page_source` 這個屬性，我們可以利用 `driver.page_source` 來取得當前瀏覽頁面的所有 HTML 標記（這並不等於網頁的原始碼）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "既然我們已經可以抓出網頁的 HTML 標記，為何還需要 BeautifulSoup 呢？\n",
    "\n",
    "`driver.page_source` 回傳的只是提取出來的字串，要以字串的形式來去檢索貼文內容並不方便。BeautifulSoup 可以解析 HTML，從字串轉換為 BeautifulSoup 物件。如此一來，我們就可以使用 BeautifulSoup 提供的方法來查找網頁元素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "page_content = driver.page_source\n",
    "\n",
    "# 因為要解析的對象是 HTML，所以需指定 html.parser 解析器\n",
    "soup = BeautifulSoup(page_content, 'html.parser')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "將 HTML 標記轉換為 BeautifulSoup 物件後，我們就可以使用 BeautifulSoup 裡的 `select` 方法。這個方法會根據 CSS Selector 來選取 HTML 的元素。\n",
    "\n",
    "輸出的結果會變成一個列表，包含了所有符合 CSS Selector 的元素。\n",
    "\n",
    "如何撰寫 CSS Selector 可參考此篇文章：https://medium.com/@xcswap.john/爬蟲必備的html-css-selectors基礎-0dc9bc399fd6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![soup_select_post](./image/soup_select_post.png)\n",
    "\n",
    "為了抓到更精確的貼文資訊、我們可以先抓每篇貼文的整體元素，後續再對每篇貼文爬取作者、時間、內容...等資訊。由上圖可以發現，社團貼文的類名（classname）是 `x1yztbdb x1n2onr6 xh8yej3 x1ja2u2z`，因此 CSS Selector 可以寫成 `.x1yztbdb.x1n2onr6.xh8yej3.x1ja2u2z`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elements = soup.select('.x1yztbdb.x1n2onr6.xh8yej3.x1ja2u2z')\n",
    "\n",
    "# 輸出隨便一篇貼文元素看看\n",
    "elements[3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 爬取貼文資訊"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 01. 貼文作者  \n",
    "\n",
    "情境：目標元素的 HTML Element 不包含屬性值。\n",
    "解法：加入目標元素的父級。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![author_position](./image/author_position.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以觀察到「作者」的網頁元素位置在 `span` 標籤下，由於沒有標記任何屬性（class 或 id），直接指定 `span` 的話很容易抓到錯誤的目標。\n",
    "\n",
    "這時，就需要給 CSS Selectors 更多的線索來找到目標的元素。\n",
    "\n",
    "以目標元素的位置為中心，往前找到最近的「父級元素」。\n",
    "\n",
    "> 只要是把目標元素包裹在自己的標籤下，都算是目標元素的父級！\n",
    "> 當然距離越遠，關係越遠，越容易受到其他節點影響喔\n",
    "\n",
    "在這個案例中，節點的關係為：\n",
    "\n",
    "span.xt0psk2（父級 3） > a.x1i10hfl.xjbqb8w...（父級 2） > strong（父級 1） > span（目標）\n",
    "\n",
    "1. `strong` 標籤不具屬性值，不是最好的選擇。\n",
    "2. `a` 具有屬性值，但 classname 太多，太過於具體反而不好維護（網站更新後較容易失效）。\n",
    "3. `span.xt0psk2` classname 不多，且其他地方沒有重複的屬性值，因此偏好加入這個父級到 CSS Selector。\n",
    "\n",
    "當然，其實使用第二步的 `a` 標籤還是可以達到同樣的效果！不過如果能用更短、更精簡的 CSS Selector，可讀性及效率會較高。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in elements:\n",
    "    try:\n",
    "        name = element.select('.xt0psk2 span')[0].text\n",
    "        \n",
    "        # 👇使用 a 標籤的效果一樣，但 CSS Selector 相比之下長很多\n",
    "        # name = element.select('.x1i10hfl.xjbqb8w.x1ejq31n.xd10rxx.x1sy0etr.x17r0tee.x972fbf.xcfux6l.x1qhh985.xm0m39n.x9f619.x1ypdohk.xt0psk2.xe8uvvx.xdj266r.x11i5rnm.xat24cr.x1mh8g0r.xexx8yu.x4uap5.x18d9i69.xkhd6sd.x16tdsg8.x1hl2dhg.xggy1nq.x1a2a7pz.xt0b8zv.xzsf02u.x1s688f span')[0].text\n",
    "        print(name)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 02. 貼文按讚數\n",
    "\n",
    "情境：目標元素的 HTML Element 在其他節點重名了（標籤與 classname 相同）。\n",
    "解法：加入目標元素的父級。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![like_position](./image/like_position.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "「貼文讚數」對應的網頁元素是 classname 為 `x1e558r4` 的 `span` 標籤。那麼我們就直接指定 `span.x1e558r4span.x1e558r4` 看看效果如何。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in elements[:2]:\n",
    "    try:\n",
    "        like = element.select('span.x1e558r4')[0].text\n",
    "        print(element.select('span.x1e558r4')[0]) \n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "從結果發現抓到的元素不是我們要的。\n",
    "\n",
    "如上圖，抓到的是其他相同標籤與 classname 的元素，所以我們需要更多關於目標元素的資訊。\n",
    "\n",
    "與前面的例子相同，在加入父級元素後，指定 CSS Selector 為 `.xt0b8zv.x2bj2ny.xrbpyxo.xl423tq span.x1e558r4`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in elements:\n",
    "    try:\n",
    "        like = element.select('.xt0b8zv.x2bj2ny.xrbpyxo.xl423tq span.x1e558r4')[0].text\n",
    "        print(like)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這樣便順利抓到我們要的結果！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 03. 貼文發佈時間\n",
    "\n",
    "情境：目標元素的 HTML Element 不包含屬性值。\n",
    "解法：加入目標元素的父級。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![time_position](./image/time_position.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "這邊的 CSS Selector 的指定邏輯與「貼文作者」一樣，但周圍父級元素的 classname 都不短，所以就直接挑最近的父級元素來指定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in elements:\n",
    "    try:\n",
    "        date_time = element.select('.x1i10hfl.xjbqb8w.x1ejq31n.xd10rxx.x1sy0etr.x17r0tee.x972fbf.xcfux6l.x1qhh985.xm0m39n.x9f619.x1ypdohk.xt0psk2.xe8uvvx.xdj266r.x11i5rnm.xat24cr.x1mh8g0r.xexx8yu.x4uap5.x18d9i69.xkhd6sd.x16tdsg8.x1hl2dhg.xggy1nq.x1a2a7pz.x1heor9g.xt0b8zv.xo1l8bm span')[0].text\n",
    "        print(date_time)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 04. 貼文連結\n",
    "\n",
    "情境：爬取目標是 `href` 屬性。\n",
    "解法：使用 `get` 來抓指定屬性的屬性值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![url_position](./image/url_position.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正好 `a` 標籤下的 `href` 屬性便是貼文連結。直接指定該元素的 classname 後，使用 `get('href')`抓出貼文連結。\n",
    "\n",
    "Facebook 的社團貼文連結結構是 https://www.facebook.com/groups/『社團 id』/posts/『貼文 id』。\n",
    "\n",
    "從畫面可以看到貼文連結非常長，而我們只需要連結的主要部分即可，所以這邊利用正規表達式，把參數前的部分連結擷取出來即可。\n",
    "\n",
    "> 實際把長連結放進瀏覽器中，一樣會重定向到「https://www.facebook.com/groups/『社團 id』/posts/『貼文 id』」的連結結構。該參數多半是 FB 內部用來追蹤用戶行為或其他用途，因為不確定參數的用途，保險起見過濾掉較好\n",
    "> 有了貼文連結後，要補充更多資料就更容易了！例如：爬取留言。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "for element in elements:\n",
    "    try:\n",
    "        link = element.select('.x1i10hfl.xjbqb8w.x1ejq31n.xd10rxx.x1sy0etr.x17r0tee.x972fbf.xcfux6l.x1qhh985.xm0m39n.x9f619.x1ypdohk.xt0psk2.xe8uvvx.xdj266r.x11i5rnm.xat24cr.x1mh8g0r.xexx8yu.x4uap5.x18d9i69.xkhd6sd.x16tdsg8.x1hl2dhg.xggy1nq.x1a2a7pz.x1heor9g.xt0b8zv.xo1l8bm')[0].get('href')\n",
    "        # 正規表達式\n",
    "        base_url = re.match(r\"(https://www\\.facebook\\.com/groups/traveler168/posts/\\d+)\", link)\n",
    "\n",
    "        if base_url:\n",
    "            print(base_url.group(1))\n",
    "        else:\n",
    "            print(link)\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 04. 貼文內容\n",
    "\n",
    "情境：欲爬取的目標文字散落在各個網頁節點下。\n",
    "解法：使用 `get_text` 來抓剛元素下的所有文本。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./image/content_position.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由上圖可以發現，Facebook 貼文內容文字分散在各個 HTML Element 內，由於每篇文的結構未必相同。因此很難一個個抓出來。\n",
    "\n",
    "此時，可以使用 `get_text` 的方法來抓出網頁元素下的所有文本，這樣就不用考慮內文的元素有哪些了。\n",
    "\n",
    "> `select_one` 的方法會回傳第一個符合 CSS Selector 的元素，所以不需要加入 `[0]` 索引出第一筆。\n",
    "> 換言之，這邊也可以改回使用 `select`！只是一定要所引出第一個元素，否則無法使用 `get_text`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for element in elements:\n",
    "    try:\n",
    "        article = element.select_one('.x193iq5w.xeuugli.x13faqbe.x1vvkbs.x1xmvt09.x1lliihq.x1s928wv.xhkezso.x1gmr53x.x1cpjm7i.x1fgarty.x1943h6x.xudqn12.x3x7a5m.x6prxxf.xvq8zen.xo1l8bm.xzsf02u.x1yc453h')\n",
    "        print(article.get_text())\n",
    "    except:\n",
    "        continue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們把前面的爬蟲程式組合在一起，建構成函式。輸出成 DataFrame 的格式來看一下結果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def getPostData(elements):\n",
    "    name_list, like_list, date_list, link_list, article_list = [], [], [], [], []\n",
    "    for element in elements:\n",
    "        try:\n",
    "            name = element.select('.xt0psk2 span')[0].text\n",
    "            name_list.append(name)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            like = element.select('.xt0b8zv.x2bj2ny.xrbpyxo.xl423tq span.x1e558r4')[0].text\n",
    "            like_list.append(like)\n",
    "        except:\n",
    "            like_list.append(0)\n",
    "\n",
    "        try:\n",
    "            date_time = element.select('.x1i10hfl.xjbqb8w.x1ejq31n.xd10rxx.x1sy0etr.x17r0tee.x972fbf.xcfux6l.x1qhh985.xm0m39n.x9f619.x1ypdohk.xt0psk2.xe8uvvx.xdj266r.x11i5rnm.xat24cr.x1mh8g0r.xexx8yu.x4uap5.x18d9i69.xkhd6sd.x16tdsg8.x1hl2dhg.xggy1nq.x1a2a7pz.x1heor9g.xt0b8zv.xo1l8bm span')[0].text\n",
    "            date_list.append(date_time)\n",
    "        except:\n",
    "            date_list.append('')\n",
    "\n",
    "        try:\n",
    "            link = element.select('.x1i10hfl.xjbqb8w.x1ejq31n.xd10rxx.x1sy0etr.x17r0tee.x972fbf.xcfux6l.x1qhh985.xm0m39n.x9f619.x1ypdohk.xt0psk2.xe8uvvx.xdj266r.x11i5rnm.xat24cr.x1mh8g0r.xexx8yu.x4uap5.x18d9i69.xkhd6sd.x16tdsg8.x1hl2dhg.xggy1nq.x1a2a7pz.x1heor9g.xt0b8zv.xo1l8bm')[0].get('href')\n",
    "            base_url = re.match(r\"(https://www\\.facebook\\.com/groups/traveler168/posts/\\d+)\", link)\n",
    "            if base_url:\n",
    "                link_list.append(base_url.group(1))\n",
    "            else:\n",
    "                link_list.append(link)\n",
    "        except:\n",
    "            link_list.append('')\n",
    "\n",
    "        try:\n",
    "            article = element.select_one('.x193iq5w.xeuugli.x13faqbe.x1vvkbs.x1xmvt09.x1lliihq.x1s928wv.xhkezso.x1gmr53x.x1cpjm7i.x1fgarty.x1943h6x.xudqn12.x3x7a5m.x6prxxf.xvq8zen.xo1l8bm.xzsf02u.x1yc453h')\n",
    "            article_list.append(article.get_text())\n",
    "        except:\n",
    "            article_list.append('')\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        '作者': name_list,\n",
    "        '讚數': like_list,\n",
    "        '時間': date_list,\n",
    "        '連結': link_list,\n",
    "        '貼文內容': article_list\n",
    "    })\n",
    "    return df\n",
    "\n",
    "post_df = getPostData(elements)\n",
    "post_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 貼文內文部分隱藏\n",
    "\n",
    "從輸出的資料表可以發現，有幾筆資料出現「查看更多」的字樣。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./image/dataframe_more.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "實際在網頁上看到的就像這個樣子，必須點擊「查看更多」才有辦法看到完整貼文內容。\n",
    "\n",
    "所以需要透過一些方式，讓瀏覽器依序點開每一篇的「查看更多」按鈕，藉此獲得完整的資料。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![more_content_sample](./image/more_content_sample.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![more_btn_position](./image/more_btn_position.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我們抓到「查看更多」按鈕的 classname 後，可以使用 `find_elements` 來指定。但是，實際上這個 classname 在很多地方大量重複，例如：貼文影片、分享的貼文區塊...等（這裡就不列出有出現的位置，感興趣的話可以檢查看看~）。\n",
    "\n",
    "為了解決這個問題，我們可以在展開全文的過程中，判斷按鈕的類型。利用 `get_attribute('textContent')` 可以抓出該元素的文字內容，如果不是「查看更多」的話，便會跳過而不點擊。\n",
    "\n",
    "> 並不是標籤名稱為 `Button` 的元素才能夠點擊喔！\n",
    "\n",
    "另外在最外層加入了 try 和 except 的語法，來進行例外處理。當 try 區段內的程式發生錯誤時，就會執行 except 裡的內容，如果 try 的程式沒有錯誤，就不會執行 except 的內容。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # 抓取所有「查看更多」按鈕的元素\n",
    "    target_class = '.x1i10hfl.xjbqb8w.x1ejq31n.xd10rxx.x1sy0etr.x17r0tee.x972fbf.xcfux6l.x1qhh985.xm0m39n.x9f619.x1ypdohk.xt0psk2.xe8uvvx.xdj266r.x11i5rnm.xat24cr.x1mh8g0r.xexx8yu.x4uap5.x18d9i69.xkhd6sd.x16tdsg8.x1hl2dhg.xggy1nq.x1a2a7pz.xt0b8zv.xzsf02u.x1s688f'\n",
    "    targets = driver.find_elements(By.CSS_SELECTOR, target_class)\n",
    "    print(f\"展開貼文：{len(targets)}\")\n",
    "    \n",
    "    # 點擊每個按鈕\n",
    "    for target in targets:\n",
    "        try:\n",
    "            # 如果谮內容為「查看更多」才會點擊，其餘跳過\n",
    "            if \"查看更多\" in target.get_attribute('textContent'):\n",
    "                target.click()\n",
    "                time.sleep(0.3)\n",
    "        except:\n",
    "            continue\n",
    "except Exception as e:\n",
    "    # 輸出錯誤訊息\n",
    "    print(e)\n",
    "    # 略過\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此時，我們再爬取一次資料看看"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getElement(driver):\n",
    "    page_content = driver.page_source\n",
    "    soup = BeautifulSoup(page_content, 'html.parser')\n",
    "    elements = soup.select('.x1yztbdb.x1n2onr6.xh8yej3.x1ja2u2z')\n",
    "    return elements\n",
    "\n",
    "elements = getElement(driver)\n",
    "\n",
    "post_df = getPostData(elements)\n",
    "post_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "檢查一下結果，看來都抓到完整的文章內容了！"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_df['貼文內容'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 過載貼文內容隱藏"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "嘗試抓30筆貼文看看結果如何"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.refresh()\n",
    "\n",
    "amount = 0\n",
    "while amount < 30:\n",
    "    \n",
    "    counter = 0\n",
    "    while counter <= 3:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(0.5)\n",
    "        counter += 1\n",
    "\n",
    "    elements = getElement(driver)\n",
    "    amount = len(elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    target_class = '.x1i10hfl.xjbqb8w.x1ejq31n.xd10rxx.x1sy0etr.x17r0tee.x972fbf.xcfux6l.x1qhh985.xm0m39n.x9f619.x1ypdohk.xt0psk2.xe8uvvx.xdj266r.x11i5rnm.xat24cr.x1mh8g0r.xexx8yu.x4uap5.x18d9i69.xkhd6sd.x16tdsg8.x1hl2dhg.xggy1nq.x1a2a7pz.xt0b8zv.xzsf02u.x1s688f'\n",
    "    targets = driver.find_elements(By.CSS_SELECTOR, target_class)\n",
    "    print(f\"展開貼文：{len(targets)}\")\n",
    "    \n",
    "    for target in targets:\n",
    "        try:\n",
    "            if \"查看更多\" in target.get_attribute('textContent'):\n",
    "                target.click()\n",
    "                time.sleep(0.3)\n",
    "        except:\n",
    "            continue\n",
    "except Exception as e:\n",
    "    print(e)\n",
    "    pass\n",
    "\n",
    "elements = getElement(driver)\n",
    "\n",
    "post_df = getPostData(elements)\n",
    "post_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最後輸出的資料居然只有12筆？但是執行 `len(elements)` 會發現實際載入的貼文確實超過30筆"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "檢查網頁元素後發現，前面載入的20多篇貼文，都被加上的 `hidden` 隱藏起來了，所以爬蟲才會抓不到。\n",
    "\n",
    "像這種以不斷滾動來載入文章內容的網頁，多少都會有這種類似的機制。為了避免過度消耗瀏覽器的記憶體和處理資源，會透過一些方式去降低資料消耗。以 Facebook 的做法，就是將不在可視範圍內（viewport）中的貼文標記為 `hidden`。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./image/hidden.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "既然不能一次載入完爬取，我們可以嘗試一邊滾動頁面，一邊爬取貼文資料。在每一輪滾動完頁面後，就儲存一次當前頁面的貼文資料。\n",
    "\n",
    "但是如果今天要爬取大量的資料，例如：100筆、1000筆。即便大部分元素的主要內容會被隱藏，但也會有100、1000個沒有意義的網頁節點保留在頁面上，爬蟲執行的時間也會變得非常長。此時，我們可以借助 Javascript 的語法，在不斷載入頁面與爬取的過程中，同時刪除已標記 hidden 的元素"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 腳本的邏輯是只要有抓到子級存在 hidden 的元素，就刪除離子級最近的貼文元素（.x1yztbdb.x1n2onr6.xh8yej3.x1ja2u2z，也就是他自己）\n",
    "js_script =  '''\n",
    "    let elements = document.querySelectorAll(\".x9f619.x1n2onr6.x1ja2u2z.x1s85apg[hidden]\");\n",
    "    for (let element of elements) {\n",
    "        let parentElement = element.closest(\".x1yztbdb.x1n2onr6.xh8yej3.x1ja2u2z\");\n",
    "        parentElement.parentNode.removeChild(parentElement);\n",
    "    }\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "執行 Javascript 腳本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.execute_script(js_script)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 完整爬蟲程式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.refresh()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame()\n",
    "\n",
    "amount = 0\n",
    "while amount < 50:\n",
    "    \n",
    "    counter = 0\n",
    "    while counter <= 3:\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(0.5)\n",
    "        counter += 1\n",
    "\n",
    "    try:\n",
    "        target_class = '.x1i10hfl.xjbqb8w.x1ejq31n.xd10rxx.x1sy0etr.x17r0tee.x972fbf.xcfux6l.x1qhh985.xm0m39n.x9f619.x1ypdohk.xt0psk2.xe8uvvx.xdj266r.x11i5rnm.xat24cr.x1mh8g0r.xexx8yu.x4uap5.x18d9i69.xkhd6sd.x16tdsg8.x1hl2dhg.xggy1nq.x1a2a7pz.xt0b8zv.xzsf02u.x1s688f'\n",
    "        targets = driver.find_elements(By.CSS_SELECTOR, target_class)\n",
    "        \n",
    "        for target in targets:\n",
    "            try:\n",
    "                if \"查看更多\" in target.get_attribute('textContent'):\n",
    "                    target.click()\n",
    "                    time.sleep(0.3)\n",
    "            except:\n",
    "                continue\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass\n",
    "\n",
    "    elements = getElement(driver)\n",
    "    post_df = getPostData(elements)\n",
    "\n",
    "    # 每一輪爬取的資料儲存進 result_df（與 result_df 合併）\n",
    "    result_df = pd.concat([result_df, post_df])\n",
    "\n",
    "    time.sleep(0.3)\n",
    "\n",
    "    driver.execute_script(js_script)\n",
    "\n",
    "    amount = len(result_df[~result_df['貼文內容'].str.contains('查看更多')]['連結'].unique())\n",
    "    print(f\"爬取進度: {amount} 筆\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因為是一邊滾動一邊抓資料，貼文有可能會重複抓取，所以我們依據「連結」來篩選出不重複的資料。\n",
    "\n",
    "不過，為什麼明明爬蟲時，展開每篇貼文的隱藏內容，還會抓到沒展開的貼文？\n",
    "\n",
    "仔細觀察展開完整貼文時 Webdriver 的變化，可以發現 Webdriver 會將畫面移動至該篇文出現在可視範圍內。假設現在有一篇需要展開的貼文，恰好位在網頁的最後幾個元素上，那麼當 Webdriver 移動到該位置（頁面底部）並進行點擊時，便會自動刷出新的貼文。若新載入的貼文正好是需要展開的貼文，那麼就會抓到未展開的貼文。\n",
    "\n",
    "畢竟需要點擊的貼文，在貼文自動刷新前就決定好了~\n",
    "\n",
    "不過這問題影響不大，下一輪滾動完後，這一篇文就會進到需要被點擊的貼文清單了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 篩選掉「貼文內容」有出現「查看更多」字樣的資料\n",
    "dataset = result_df[~result_df['貼文內容'].str.contains('查看更多')]\n",
    "\n",
    "# drop_duplicates 會依據「連結」篩除重覆的，只保留第一筆出現的資料\n",
    "dataset = dataset.drop_duplicates(subset = '連結').reset_index(drop = True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "將資料儲存成 csv 格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv('./data/fb_post_01.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
